{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig\n",
    "import os\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from lightning_pose.data.dali import PrepareDALI\n",
    "from lightning_pose.utils.io import ckpt_path_from_base_path\n",
    "from lightning_pose.utils.predictions import (\n",
    "    get_cfg_file, \n",
    "    predict_single_video, \n",
    "    load_model_from_checkpoint, \n",
    "    PredictionHandler,\n",
    ")\n",
    "from lightning_pose.utils.scripts import (\n",
    "    compute_metrics,\n",
    "    get_imgaug_transform, \n",
    "    get_dataset, \n",
    "    get_data_module,\n",
    ")\n",
    "\n",
    "from eks.utils import convert_lp_dlc, make_output_dataframe, populate_output_dataframe\n",
    "from eks.singlecam_smoother import ensemble_kalman_smoother_singlecam\n",
    "\n",
    "from pseudo_labeler.utils import format_data_walk\n",
    "from pseudo_labeler.evaluation import compute_likelihoods_and_variance\n",
    "\n",
    "dataset_name = \"mirror-mouse\"\n",
    "n_hand_labels = 100\n",
    "n_pseudo_labels = 1000\n",
    "seeds=[0, 1, 2, 3]\n",
    "selection_strategy=\"random\"\n",
    "# dataset_name = \"mirror-fish\"\n",
    "# dataset_name = \"crim13\"\n",
    "\n",
    "# where the labeled data is stored\n",
    "data_dir = f\"/teamspace/studios/this_studio/data/{dataset_name}\"\n",
    "\n",
    "# where the video snippets are stored\n",
    "snippets_dir = f\"{data_dir}/videos-for-each-labeled-frame\"\n",
    "\n",
    "# where the network models are stored\n",
    "networks_dir = f\"/teamspace/studios/this_studio/outputs/{dataset_name}/hand={n_hand_labels}_pseudo={n_pseudo_labels}/networks\"\n",
    "\n",
    "# where to save eks/post-processor outputs\n",
    "eks_save_dir = f\"/teamspace/studios/this_studio/outputs/{dataset_name}/hand={n_hand_labels}_pseudo={n_pseudo_labels}/post-processors/eks_rng={seeds[0]}-{seeds[-1]}/eks_ood_snippets\"\n",
    "\n",
    "aeks_dir = f\"/teamspace/studios/this_studio/outputs/{dataset_name}/hand={n_hand_labels}_pseudo={n_pseudo_labels}/results_aeks_{selection_strategy}\"\n",
    "\n",
    "# where to save aeks_eks outputs\n",
    "aeks_eks_save_dir = f\"/teamspace/studios/this_studio/outputs/{dataset_name}/hand={n_hand_labels}_pseudo={n_pseudo_labels}/results_amortized_eks/eks_rng={seeds[0]}-{seeds[-1]}/eks\"\n",
    "\n",
    "# where model configs are stored\n",
    "config_dir = f\"/teamspace/studios/this_studio/keypoint-pseudo-labeler/configs/\"\n",
    "\n",
    "# file name of csv where marker data is stored\n",
    "ground_truth_csv = 'CollectedData_new.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_model_dirs(base_dir, keyword):\n",
    "    model_dirs = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for dir_name in dirs:\n",
    "            if keyword in dir_name:\n",
    "                model_dirs.append(os.path.join(root, dir_name))\n",
    "    return model_dirs\n",
    "\n",
    "model_dirs_list = []\n",
    "if dataset_name == \"mirror-mouse\":\n",
    "    model_dirs_list = find_model_dirs(networks_dir, 'rng')\n",
    "    print(f\"Found {len(model_dirs_list)} model directories\")\n",
    "    keypoint_ensemble_list = [\n",
    "        'paw1LH_top', 'paw2LF_top', 'paw3RF_top', 'paw4RH_top', 'nose_top', 'tailBase_top', 'tailMid_top',\n",
    "        'paw1LH_bot', 'paw2LF_bot', 'paw3RF_bot', 'paw4RH_bot', 'nose_bot', 'tailBase_bot', 'tailMid_bot',\n",
    "    ]\n",
    "else:\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd6459",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0331c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_for_sessions_and_frames(ground_truth_df):\n",
    "    session_frame_info = []\n",
    "    for idx, row in tqdm(ground_truth_df.iterrows(), total=len(ground_truth_df)):\n",
    "        first_col_value = row[0]\n",
    "        path_parts = first_col_value.split('/')\n",
    "        session = path_parts[1]\n",
    "        frame = path_parts[2]\n",
    "        session_frame_info.append((session, frame))\n",
    "    return session_frame_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6c377",
   "metadata": {},
   "source": [
    "## Step 1: run inference on video snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd303239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_snippets(model_dirs_list, data_dir, snippets_dir, ground_truth_df):\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", devices=1)\n",
    "    session_frame_info = process_csv_for_sessions_and_frames(ground_truth_df)\n",
    "    \n",
    "    for model_dir in model_dirs_list:\n",
    "        print(f'Processing model in {os.path.basename(model_dir)}')\n",
    "        cfg_file = os.path.join(model_dir, \"config.yaml\")\n",
    "        model_cfg = DictConfig(yaml.safe_load(open(cfg_file)))\n",
    "        \n",
    "        # Update data directories\n",
    "        model_cfg.data.data_dir = data_dir\n",
    "        model_cfg.data.video_dir = os.path.join(data_dir, \"videos\")\n",
    "        \n",
    "        # Get model checkpoint\n",
    "        ckpt_file = ckpt_path_from_base_path(model_dir, model_name=model_cfg.model.model_name)\n",
    "        \n",
    "        # Build datamodule\n",
    "        cfg_new = model_cfg.copy()\n",
    "        cfg_new.training.imgaug = 'default'\n",
    "        imgaug_transform = get_imgaug_transform(cfg=cfg_new)\n",
    "        dataset_new = get_dataset(cfg=cfg_new, data_dir=cfg_new.data.data_dir, imgaug_transform=imgaug_transform)\n",
    "        datamodule_new = get_data_module(cfg=cfg_new, dataset=dataset_new, video_dir=cfg_new.data.video_dir)\n",
    "        datamodule_new.setup()\n",
    "        \n",
    "        # Load model\n",
    "        model = load_model_from_checkpoint(cfg=cfg_new, ckpt_file=ckpt_file, eval=True, data_module=datamodule_new)\n",
    "        model.to(\"cuda\")\n",
    "        \n",
    "        model_cfg.dali.base.sequence_length = 16\n",
    "\n",
    "        for session, frame in tqdm(session_frame_info):\n",
    "            video_file = os.path.join(snippets_dir, session, frame.replace('png', 'mp4'))\n",
    "            prediction_csv_file = os.path.join(model_dir, \"video_preds_labeled\", session, frame.replace('png', 'csv'))\n",
    "            os.makedirs(os.path.join(model_dir, \"video_preds_labeled\", session), exist_ok=True)\n",
    "            \n",
    "            if not os.path.exists(video_file):\n",
    "                print(f'Cannot find video snippet for {video_file}. Skipping')\n",
    "                continue\n",
    "            if os.path.exists(prediction_csv_file):\n",
    "                print(f'Prediction csv file already exists for {session}/{frame}. Skipping')\n",
    "                continue\n",
    "            \n",
    "            print(f'{video_file} saved as\\n{prediction_csv_file}')\n",
    "            cfg = get_cfg_file(cfg_file=cfg_new)\n",
    "            model_type = \"context\" if cfg.model.model_type == \"heatmap_mhcrnn\" else \"base\"\n",
    "            cfg.training.imgaug = \"default\"\n",
    "            \n",
    "            vid_pred_class = PrepareDALI(\n",
    "                train_stage=\"predict\",\n",
    "                model_type=model_type,\n",
    "                dali_config=cfg.dali,\n",
    "                filenames=[video_file],\n",
    "                resize_dims=[cfg.data.image_resize_dims.height, cfg.data.image_resize_dims.width]\n",
    "            )\n",
    "            \n",
    "            # Get loader\n",
    "            predict_loader = vid_pred_class()\n",
    "            \n",
    "            # Initialize prediction handler class\n",
    "            pred_handler = PredictionHandler(cfg=cfg, data_module=datamodule_new, video_file=video_file)\n",
    "            \n",
    "            # Compute predictions\n",
    "            preds = trainer.predict(model=model, dataloaders=predict_loader, return_predictions=True)\n",
    "            \n",
    "            # Process predictions\n",
    "            preds_df = pred_handler(preds=preds)\n",
    "            os.makedirs(os.path.dirname(prediction_csv_file), exist_ok=True)\n",
    "            preds_df.to_csv(prediction_csv_file)\n",
    "            \n",
    "            # Clear up memory\n",
    "            del predict_loader\n",
    "            del pred_handler\n",
    "            del vid_pred_class\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        del dataset_new\n",
    "        del datamodule_new\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd274ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = pd.read_csv(os.path.join(data_dir, ground_truth_csv), skiprows=2)\n",
    "run_inference_on_snippets(model_dirs_list, data_dir, snippets_dir, ground_truth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77ca29",
   "metadata": {},
   "source": [
    "## Step 2: run EKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eks_on_snippets(snippets_dir, model_dirs_list, eks_save_dir, ground_truth_df):\n",
    "    session_frame_info = process_csv_for_sessions_and_frames(ground_truth_df)\n",
    "    tracker_name = 'heatmap_tracker'\n",
    "    keypoint_names = None\n",
    "\n",
    "    # store useful info here\n",
    "    index_list = []\n",
    "    results_list = []\n",
    "\n",
    "    # Step 1: Process and save the CSVs\n",
    "    for session, frame in session_frame_info:\n",
    "        # extract all markers\n",
    "        markers_list = []\n",
    "        for model_dir in model_dirs_list:\n",
    "            csv_file = os.path.join(\n",
    "                model_dir, 'video_preds_labeled', session, frame.replace('png', 'csv'))\n",
    "            df_tmp = pd.read_csv(csv_file, header=[0, 1, 2], index_col=0)\n",
    "            keypoint_names = [l[1] for l in df_tmp.columns[::3]]\n",
    "            markers_tmp = convert_lp_dlc(df_tmp, keypoint_names, model_name=tracker_name)\n",
    "            markers_list.append(markers_tmp)\n",
    "\n",
    "        dfs_markers = markers_list\n",
    "        # make empty dataframe to write results into\n",
    "        df_eks = make_output_dataframe(df_tmp)\n",
    "\n",
    "        # Convert list of DataFrames to a 3D NumPy array\n",
    "        data_arrays = [df.to_numpy() for df in markers_list]\n",
    "        markers_3d_array = np.stack(data_arrays, axis=0)\n",
    "\n",
    "        # Map keypoint names to keys in input_dfs and crop markers_3d_array\n",
    "        keypoint_is = {}\n",
    "        keys = []\n",
    "        for i, col in enumerate(markers_list[0].columns):\n",
    "            keypoint_is[col] = i\n",
    "        for part in keypoint_ensemble_list:\n",
    "            keys.append(keypoint_is[part + '_x'])\n",
    "            keys.append(keypoint_is[part + '_y'])\n",
    "            keys.append(keypoint_is[part + '_likelihood'])\n",
    "        key_cols = np.array(keys)\n",
    "        markers_3d_array = markers_3d_array[:, :, key_cols]\n",
    "\n",
    "        save_dir = os.path.join(eks_save_dir, 'eks', session)\n",
    "        save_file = os.path.join(save_dir, frame.replace('png', 'csv'))\n",
    "        if os.path.exists(save_file):\n",
    "            print(f'Skipping ensembling for {session}/{frame} as it already exists.')\n",
    "            continue\n",
    "        else:\n",
    "            print(f'Ensembling for {session}/{frame}')\n",
    "            # Call the smoother function\n",
    "            df_dicts, s_finals = ensemble_kalman_smoother_singlecam(\n",
    "                markers_3d_array,\n",
    "                keypoint_ensemble_list,\n",
    "                smooth_param=None,\n",
    "                s_frames=[(None, None)],\n",
    "                blocks=[],\n",
    "            )\n",
    "            # put results into new dataframe\n",
    "            for k, keypoint_name in enumerate(keypoint_ensemble_list):\n",
    "                keypoint_df = df_dicts[k][keypoint_name + '_df']\n",
    "                df_eks = populate_output_dataframe(keypoint_df, keypoint_name, df_eks)\n",
    "\n",
    "            # save eks results\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            df_eks.to_csv(save_file)\n",
    "\n",
    "    # Step 2: Extract center frame results\n",
    "    print('Extracting center frame results from all sessions.')\n",
    "    for session, frame in session_frame_info:\n",
    "        # Construct the path to the saved EKS results\n",
    "        save_file = os.path.join(eks_save_dir, 'eks', session, frame.replace('png', 'csv'))\n",
    "        if not os.path.exists(save_file):\n",
    "            print(f'Missing EKS file for {session}/{frame}. Skipping.')\n",
    "            continue\n",
    "\n",
    "        # read csv\n",
    "        df_eks = pd.read_csv(save_file, header=[0, 1, 2], index_col=0)\n",
    "        \n",
    "        # extract result from center frame\n",
    "        assert df_eks.shape[0] & 2 != 0\n",
    "        idx_frame = int(np.floor(df_eks.shape[0] / 2))\n",
    "        frame_file = frame.replace('.mp4', '.png')\n",
    "        index_name = f'labeled-data/{session}/{frame_file}'\n",
    "        result = df_eks[df_eks.index == idx_frame].rename(index={idx_frame: index_name})\n",
    "        results_list.append(result)\n",
    "\n",
    "    # save final predictions\n",
    "    results_df = pd.concat(results_list)\n",
    "    results_df.sort_index(inplace=True)\n",
    "    # add \"set\" column so this df is interpreted as labeled data predictions\n",
    "    results_df.loc[:, (\"set\", \"\", \"\")] = \"train\"\n",
    "    results_df.to_csv(os.path.join(eks_save_dir, 'eks', 'predictions_new.csv'))\n",
    "\n",
    "    return df_eks, dfs_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e366968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eks, dfs_markers = run_eks_on_snippets(snippets_dir, model_dirs_list, eks_save_dir, ground_truth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41400e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfs_markers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bfc6b0",
   "metadata": {},
   "source": [
    "## Step 2.5: sanity check EKS dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ce0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "def plot_eks_dfs(keypoint_ensemble_list, markers_list, df_eks):\n",
    "    \"\"\"\n",
    "    Plots the EKS DataFrames for the given keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - keypoint_ensemble_list: List of keypoints to plot.\n",
    "    - markers_list: List of DataFrames containing marker data.\n",
    "    - df_eks: DataFrame containing the ensemble kalman smoother results.\n",
    "    \"\"\"\n",
    "    for kp in keypoint_ensemble_list:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "        for ax, coord in zip(axes, ['x', 'y']):\n",
    "            cols0 = (f'{kp}_{coord}')\n",
    "            cols1 = ('ensemble-kalman_tracker', kp, coord)\n",
    "            for i, df in enumerate(markers_list):\n",
    "                m = df.loc[:, cols0]\n",
    "                color = [0.5, 0.5, 0.5] # if i < 5 else [0, 0, 1]  # Use grey for rng0-4 and red for rng5 and beyond\n",
    "                ax.plot(m.to_numpy(), color=color)\n",
    "            m = df_eks.loc[:, cols1]\n",
    "            ax.plot(m, color='k')\n",
    "            ax.set_title(cols0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eks_dfs(keypoint_ensemble_list, dfs_markers, df_eks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dca8e4",
   "metadata": {},
   "source": [
    "## Step 3: compute ens mean/median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40ec35",
   "metadata": {},
   "source": [
    "### collect predictions from individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2580ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_preds(model_dirs_list, snippets_dir):\n",
    "    for model_dir in tqdm(model_dirs_list):\n",
    "        results_list = []\n",
    "        sessions = os.listdir(snippets_dir)\n",
    "        for session in sessions:\n",
    "            frames = os.listdir(os.path.join(snippets_dir, session))\n",
    "            for frame in frames:\n",
    "                # Load prediction on snippet\n",
    "                df = pd.read_csv(\n",
    "                    os.path.join(model_dir, 'video_preds_labeled', session, frame.replace('.mp4', '.csv')), \n",
    "                    header=[0, 1, 2], \n",
    "                    index_col=0,\n",
    "                )\n",
    "                # Extract result from center frame\n",
    "                assert df.shape[0] & 2 != 0\n",
    "                idx_frame = int(np.floor(df.shape[0] / 2))\n",
    "                frame_file = frame.replace('.mp4', '.png')\n",
    "                index_name = f'labeled-data/{session}/{frame_file}'\n",
    "                result = df[df.index == idx_frame].rename(index={idx_frame: index_name})\n",
    "                results_list.append(result)\n",
    "\n",
    "        # Save final predictions\n",
    "        results_df = pd.concat(results_list)\n",
    "        results_df.sort_index(inplace=True)\n",
    "        # Add \"set\" column so this df is interpreted as labeled data predictions\n",
    "        results_df.loc[:, (\"set\", \"\", \"\")] = \"train\"\n",
    "        results_df.to_csv(os.path.join(model_dir, 'video_preds_labeled', 'predictions_new.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9121f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_preds(model_dirs_list, snippets_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b56f1",
   "metadata": {},
   "source": [
    "### compute ens mean/median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de16906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ens_mean_median(model_dirs_list, eks_save_dir, post_processor_type):\n",
    "    markers_list = []\n",
    "    for model_dir in model_dirs_list:\n",
    "        csv_file = os.path.join(model_dir, 'video_preds_labeled', 'predictions_new.csv')\n",
    "        df_tmp = pd.read_csv(csv_file, header=[0, 1, 2], index_col=0)\n",
    "        preds_curr = df_tmp.to_numpy()[:, :-1]  # remove \"set\" column\n",
    "        preds_curr = np.delete(preds_curr, list(range(2, preds_curr.shape[1], 3)), axis=1)\n",
    "        preds_curr = np.reshape(preds_curr, (preds_curr.shape[0], -1, 2))\n",
    "        markers_list.append(preds_curr[..., None])\n",
    "    \n",
    "    # Concatenate across last dim\n",
    "    pred_arrays = np.concatenate(markers_list, axis=3)\n",
    "    \n",
    "    # Compute mean/median across x/y\n",
    "    if post_processor_type == 'ens-mean':\n",
    "        ens_mean = np.mean(pred_arrays, axis=3)\n",
    "    elif post_processor_type == 'ens-median':\n",
    "        ens_mean = np.median(pred_arrays, axis=3)\n",
    "    \n",
    "    ens_likelihood = np.nan * np.zeros((ens_mean.shape[0], ens_mean.shape[1], 1))\n",
    "    \n",
    "    # Build dataframe\n",
    "    xyl = np.concatenate([ens_mean, ens_likelihood], axis=2)\n",
    "    df_final = pd.DataFrame(\n",
    "        xyl.reshape(ens_mean.shape[0], -1), \n",
    "        columns=df_tmp.columns[:-1],  # remove \"set\" column, add back in later\n",
    "        index=df_tmp.index\n",
    "    )\n",
    "    df_final.sort_index(inplace=True)\n",
    "    # Add \"set\" column so this df is interpreted as labeled data predictions\n",
    "    df_final.loc[:, (\"set\", \"\", \"\")] = \"train\"\n",
    "    save_dir_ = os.path.join(eks_save_dir, f'{post_processor_type}')\n",
    "    os.makedirs(save_dir_, exist_ok=True)\n",
    "    df_final.to_csv(os.path.join(save_dir_, 'predictions_new.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ens_mean_median(model_dirs_list, eks_save_dir, 'ens-mean')\n",
    "compute_ens_mean_median(model_dirs_list, eks_save_dir, 'ens-median')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec9142",
   "metadata": {},
   "source": [
    "## Step 4: compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cdcdd",
   "metadata": {},
   "source": [
    "### step 4.1: create data module to have access to labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = os.path.join(config_dir, f\"config_{dataset_name}.yaml\")\n",
    "cfg = DictConfig(yaml.safe_load(open(cfg_file)))\n",
    "\n",
    "model_cfg = cfg.copy()\n",
    "\n",
    "model_cfg.data.data_dir = data_dir\n",
    "model_cfg.data.csv_file = ground_truth_csv\n",
    "\n",
    "model_cfg.training.imgaug = \"default\"\n",
    "model_cfg.training.train_prob = 1\n",
    "model_cfg.training.val_prob = 0\n",
    "model_cfg.training.train_frames = 1\n",
    "\n",
    "imgaug_transform = get_imgaug_transform(cfg=model_cfg)\n",
    "dataset = get_dataset(\n",
    "    cfg=model_cfg, \n",
    "    data_dir=model_cfg.data.data_dir, \n",
    "    imgaug_transform=imgaug_transform)\n",
    "data_module = get_data_module(\n",
    "    cfg=model_cfg, \n",
    "    dataset=dataset, \n",
    "    video_dir=os.path.join(data_dir, dataset_name, \"videos\"))\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebb95e",
   "metadata": {},
   "source": [
    "### step 4.2: compute metrics on ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir in model_dirs_list:\n",
    "    print(model_dir)\n",
    "    preds_file = os.path.join(model_dir, 'video_preds_labeled', 'predictions_new.csv')\n",
    "    compute_metrics(cfg=model_cfg, preds_file=preds_file, data_module=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf9735",
   "metadata": {},
   "source": [
    "### step 4.3: compute metrics on post-processed traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor_types = [\n",
    "    \"ens-mean\",\n",
    "    \"ens-median\",\n",
    "    \"eks\",\n",
    "]\n",
    "\n",
    "for post_processor_type in post_processor_types:\n",
    "    print(f'{post_processor_type}')\n",
    "    preds_file = os.path.join(eks_save_dir, f'{post_processor_type}', 'predictions_new.csv')\n",
    "    compute_metrics(cfg=model_cfg, preds_file=preds_file, data_module=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420bb61",
   "metadata": {},
   "source": [
    "## Step 5: run everything again with expanded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fe191",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs_list = []\n",
    "if dataset_name == \"mirror-mouse\":\n",
    "    model_dirs_list = find_model_dirs(aeks_dir, 'rng')\n",
    "    keypoint_ensemble_list = [\n",
    "        'paw1LH_top', 'paw2LF_top', 'paw3RF_top', 'paw4RH_top', 'nose_top', 'tailBase_top', 'tailMid_top',\n",
    "        'paw1LH_bot', 'paw2LF_bot', 'paw3RF_bot', 'paw4RH_bot', 'nose_bot', 'tailBase_bot', 'tailMid_bot',\n",
    "    ]\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# remove eks_rng=0-3\n",
    "for directory in model_dirs_list:\n",
    "    if 'eks' in os.path.basename(directory):\n",
    "        print(f\"{directory} has 'eks' in it. Removing\")\n",
    "        model_dirs_list.remove(directory)\n",
    "print(f\"Found {len(model_dirs_list)} model directories\")\n",
    "\n",
    "# 1. running inference\n",
    "run_inference_on_snippets(model_dirs_list, data_dir, snippets_dir, ground_truth_df)\n",
    "\n",
    "# 2. running EKS\n",
    "df_eks, dfs_markers = run_eks_on_snippets(snippets_dir, model_dirs_list, aeks_eks_save_dir, ground_truth_df)\n",
    "\n",
    "# 3.1 collect preds from aeks models\n",
    "collect_preds(model_dirs_list, snippets_dir)\n",
    "# 3.2 compute ens mean and median\n",
    "compute_ens_mean_median(model_dirs_list, aeks_eks_save_dir, 'ens-mean')\n",
    "compute_ens_mean_median(model_dirs_list, aeks_eks_save_dir, 'ens-median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07834eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "# 4.1 Create Data Module\n",
    "cfg_file = os.path.join(config_dir, f\"config_{dataset_name}.yaml\")\n",
    "cfg = DictConfig(yaml.safe_load(open(cfg_file)))\n",
    "\n",
    "model_cfg = cfg.copy()\n",
    "\n",
    "model_cfg.data.data_dir = data_dir\n",
    "model_cfg.data.csv_file = ground_truth_csv\n",
    "\n",
    "model_cfg.training.imgaug = \"default\"\n",
    "model_cfg.training.train_prob = 1\n",
    "model_cfg.training.val_prob = 0\n",
    "model_cfg.training.train_frames = 1\n",
    "\n",
    "imgaug_transform = get_imgaug_transform(cfg=model_cfg)\n",
    "dataset = get_dataset(\n",
    "    cfg=model_cfg, \n",
    "    data_dir=model_cfg.data.data_dir, \n",
    "    imgaug_transform=imgaug_transform)\n",
    "data_module = get_data_module(\n",
    "    cfg=model_cfg, \n",
    "    dataset=dataset, \n",
    "    video_dir=os.path.join(data_dir, dataset_name, \"videos\"))\n",
    "data_module.setup()\n",
    "\n",
    "# 4.2 compute metrics on aEKS ensemble members\n",
    "for model_dir in model_dirs_list:\n",
    "    print(model_dir)\n",
    "    preds_file = os.path.join(model_dir, 'video_preds_labeled', 'predictions_new.csv')\n",
    "    compute_metrics(cfg=model_cfg, preds_file=preds_file, data_module=data_module)\n",
    "\n",
    "# 4.3 compute metric on post-processed traces\n",
    "post_processor_types = [\n",
    "    \"ens-mean\",\n",
    "    \"ens-median\",\n",
    "    \"eks\",\n",
    "]\n",
    "\n",
    "for post_processor_type in post_processor_types:\n",
    "    print(f'{post_processor_type}')\n",
    "    preds_file = os.path.join(aeks_eks_save_dir, f'{post_processor_type}', 'predictions_new.csv')\n",
    "    compute_metrics(cfg=model_cfg, preds_file=preds_file, data_module=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699089e",
   "metadata": {},
   "source": [
    "### step 5.1: calculate ensemble variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground truth data, for ensuring indices match\n",
    "df_gt = pd.read_csv(os.path.join(data_dir, 'CollectedData_new.csv'), header=[0, 1, 2], index_col=0)\n",
    "df_gt.sort_index(inplace=True)\n",
    "\n",
    "aeks_hand_dir = os.path.join(aeks_dir, '../results_aeks_hand')\n",
    "\n",
    "data_to_plot = {\n",
    "    'networks_rng0': os.path.join(networks_dir, 'rng0/video_preds_labeled/predictions_new.csv'),\n",
    "    'networks_rng1': os.path.join(networks_dir, 'rng1/video_preds_labeled/predictions_new.csv'),\n",
    "    'networks_rng2': os.path.join(networks_dir, 'rng2/video_preds_labeled/predictions_new.csv'),\n",
    "    'networks_rng3': os.path.join(networks_dir, 'rng3/video_preds_labeled/predictions_new.csv'),\n",
    "    #'networks_rng6': os.path.join(networks_dir, 'rng6/video_preds_labeled/predictions_new.csv'),\n",
    "    #'networks_rng5': os.path.join(networks_dir, 'rng5/video_preds_labeled/predictions_new.csv'),\n",
    "    #'networks_rng7': os.path.join(networks_dir, 'rng7/video_preds_labeled/predictions_new.csv'),\n",
    "    #'networks_rng8': os.path.join(networks_dir, 'rng8/video_preds_labeled/predictions_new.csv'),\n",
    "    #'networks_rng9': os.path.join(networks_dir, 'rng9/video_preds_labeled/predictions_new.csv'),\n",
    "    'eks': os.path.join(eks_save_dir, 'eks/predictions_new.csv'),\n",
    "    #'ens-mean': os.path.join(eks_save_dir, 'ens-mean/predictions_new.csv'),\n",
    "    #'ens-median': os.path.join(eks_save_dir, 'ens-median/predictions_new.csv'),\n",
    "    'aeks_random_rng0': os.path.join(aeks_dir, 'rng0/video_preds_labeled/predictions_new.csv'),\n",
    "    'aeks_random_rng1': os.path.join(aeks_dir, 'rng1/video_preds_labeled/predictions_new.csv'),\n",
    "    'aeks_random_rng2': os.path.join(aeks_dir, 'rng2/video_preds_labeled/predictions_new.csv'),\n",
    "    'aeks_random_rng3': os.path.join(aeks_dir, 'rng3/video_preds_labeled/predictions_new.csv'),\n",
    "    'aeks_random_eks': os.path.join(aeks_dir, 'eks_rng=0-3/eks/predictions_new.csv'),\n",
    "    'aeks_hand_rng0': os.path.join(aeks_hand_dir, 'rng0/video_preds_labeled/predictions_new.csv'),\n",
    "    'aeks_hand_rng1': os.path.join(aeks_hand_dir, 'rng1/video_preds_labeled/predictions_new.csv'),\n",
    "    'aeks_hand_rng2': os.path.join(aeks_hand_dir, 'rng2/video_preds_labeled/predictions_new.csv'),\n",
    "    'aeks_hand_rng3': os.path.join(aeks_hand_dir, 'rng3/video_preds_labeled/predictions_new.csv'),\n",
    "    'aeks_hand_eks': os.path.join(aeks_hand_dir, 'eks_rng=0-3/eks/predictions_new.csv'),\n",
    "    #'aeks_ens-mean': os.path.join(aeks_dir, 'eks_rng=0-3/ens-mean/predictions_new.csv'),\n",
    "    #'aeks_ens-median': os.path.join(aeks_dir, 'eks_rng=0-3/ens-median/predictions_new.csv'),\n",
    "}\n",
    "\n",
    "# define predictions so we can compute ensemble variance\n",
    "pred_csv_list = []\n",
    "# define model names\n",
    "model_names_list = []\n",
    "# reformat\n",
    "for key, val in data_to_plot.items():\n",
    "    model_names_list.append(key)\n",
    "    pred_csv_list.append(val)\n",
    "\n",
    "# define pixel error to plot\n",
    "error_csv_list = [p.replace('.csv', '_pixel_error.csv') for p in pred_csv_list]\n",
    "\n",
    "# load data\n",
    "df_pred_list = []\n",
    "df_error_list = []\n",
    "for pred_csv, error_csv in zip(pred_csv_list, error_csv_list):\n",
    "#     df_pred_list.append(pd.read_csv(pred_csv, header=[0, 1, 2], index_col=0).drop(columns=['set']))\n",
    "#     df_error_list.append(pd.read_csv(error_csv, header=[0], index_col=0).drop(columns=['set']))\n",
    "    \n",
    "    df_pred_list.append(pd.read_csv(pred_csv, header=[0, 1, 2], index_col=0).sort_index())\n",
    "    df_error_list.append(pd.read_csv(error_csv, header=[0], index_col=0).drop(columns=['set']).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def standardize_scorer_level(df, new_scorer='standard_scorer'):\n",
    "    \"\"\"\n",
    "    Standardizes the 'scorer' level in the MultiIndex to a common name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame to standardize.\n",
    "    new_scorer : str\n",
    "        The new name for the 'scorer' level.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame with the standardized 'scorer' level.\n",
    "    \"\"\"\n",
    "    df.columns = pd.MultiIndex.from_tuples(\n",
    "        [(new_scorer, bodypart, coord) for scorer, bodypart, coord in df.columns],\n",
    "        names=df.columns.names\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def compute_ensemble_stddev(\n",
    "    df_ground_truth,\n",
    "    df_preds,\n",
    "    keypoint_ensemble_list,\n",
    "    scorer_name='standard_scorer'\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ground_truth : List[pd.DataFrame]\n",
    "        ground truth predictions\n",
    "    df_preds : List[pd.DataFrame]\n",
    "        model predictions\n",
    "    keypoint_ensemble_list : List[str]\n",
    "        keypoints to include in the analysis\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        shape (n_frames, n_keypoints)\n",
    "    \"\"\"\n",
    "    # Initial check for NaNs in df_preds\n",
    "    for i, df in enumerate(df_preds):\n",
    "        if df.isna().any().any():\n",
    "            print(f\"Warning: NaN values detected in initial DataFrame {i}.\")\n",
    "            nan_indices = df[df.isna().any(axis=1)].index\n",
    "            nan_columns = df.columns[df.isna().any()]\n",
    "            print(f\"NaN values found at indices: {nan_indices} in columns: {nan_columns}\")\n",
    "\n",
    "    preds = []\n",
    "    cols_order = None\n",
    "    for i, df in enumerate(df_preds):\n",
    "        assert np.all(df.index == df_ground_truth.index), f\"Index mismatch between ground truth and predictions at dataframe {i}\"\n",
    "        \n",
    "        # Standardize the 'scorer' level\n",
    "        df = standardize_scorer_level(df, scorer_name)\n",
    "        \n",
    "        # Remove likelihood columns\n",
    "        cols_to_keep = [col for col in df.columns if not col[2].endswith('_likelihood') and 'zscore' not in col[2]]\n",
    "        # Keep only columns matching the keypoint_ensemble_list\n",
    "        cols_to_keep = [col for col in cols_to_keep if col[1] in keypoint_ensemble_list]\n",
    "        df = df[cols_to_keep]\n",
    "        \n",
    "        print(f\"DataFrame {i} kept columns:\", df.columns)\n",
    "        \n",
    "        # Check for NaNs in the DataFrame\n",
    "        if df.isna().any().any():\n",
    "            print(f\"Warning: NaN values detected in DataFrame {i} after filtering.\")\n",
    "            nan_indices = df[df.isna().any(axis=1)].index\n",
    "            nan_columns = df.columns[df.isna().any()]\n",
    "            print(f\"NaN values found at indices: {nan_indices} in columns: {nan_columns}\")\n",
    "        \n",
    "        # Print the order of the column headers\n",
    "        if cols_order is None:\n",
    "            cols_order = df.columns\n",
    "        else:\n",
    "            if not (df.columns == cols_order).all():\n",
    "                print(f\"Column order mismatch detected in DataFrame {i}\")\n",
    "                print(\"Expected order:\", cols_order)\n",
    "                print(\"Actual order:\", df.columns)\n",
    "                # Ensure bodyparts and coordinates are consistent\n",
    "                expected_bodyparts_coords = cols_order.droplevel(0).unique()\n",
    "                actual_bodyparts_coords = df.columns.droplevel(0).unique()\n",
    "                if not expected_bodyparts_coords.equals(actual_bodyparts_coords):\n",
    "                    print(\"Bodyparts and coordinates mismatch detected\")\n",
    "                    print(\"Expected bodyparts and coordinates:\", expected_bodyparts_coords)\n",
    "                    print(\"Actual bodyparts and coordinates:\", actual_bodyparts_coords)\n",
    "        \n",
    "        # Reshape the DataFrame to the appropriate shape\n",
    "        try:\n",
    "            arr = df.to_numpy().reshape(df.shape[0], -1, 2)\n",
    "        except ValueError as e:\n",
    "            print(f\"Reshape error: {e}\")\n",
    "            print(f\"DataFrame shape: {df.shape}\")\n",
    "            print(f\"Array shape after reshape attempt: {df.to_numpy().shape}\")\n",
    "            raise\n",
    "        \n",
    "        preds.append(arr[..., None])\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=3)\n",
    "    \n",
    "    # Check for NaNs in preds\n",
    "    if np.isnan(preds).any():\n",
    "        print(\"Warning: NaN values detected in preds array.\")\n",
    "        nan_indices = np.argwhere(np.isnan(preds))\n",
    "        print(f\"NaN values found at indices: {nan_indices}\")\n",
    "    else:\n",
    "        print(\"No NaN values detected in preds array.\")\n",
    "    \n",
    "    stddevs = np.std(preds, axis=-1).mean(axis=-1)\n",
    "    print(f\"Stddevs: {stddevs}\")\n",
    "    return stddevs\n",
    "\n",
    "def compute_percentiles(arr, std_vals, percentiles):\n",
    "    num_pts = arr[0]\n",
    "    vals = []\n",
    "    prctiles = []\n",
    "    for p in percentiles:\n",
    "        v = num_pts * p / 100\n",
    "        idx = np.argmin(np.abs(arr - v))\n",
    "        # maybe we don't have enough data\n",
    "        if idx == len(arr) - 1:\n",
    "            p_ = arr[idx] / num_pts * 100\n",
    "        else:\n",
    "            p_ = p\n",
    "        vals.append(std_vals[idx])\n",
    "        prctiles.append(p_)\n",
    "    return vals, prctiles\n",
    "\n",
    "def cleanaxis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(top=False)\n",
    "    ax.tick_params(right=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ensemble variance\n",
    "ens_stddev = compute_ensemble_stddev(\n",
    "    df_gt,\n",
    "    df_pred_list,\n",
    "    keypoint_ensemble_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record pixel errors along with ensemble variances\n",
    "df_w_vars = []\n",
    "for df_error, model_name in zip(df_error_list, model_names_list):\n",
    "    assert np.all(df_error.index == df_gt.index)\n",
    "    \n",
    "    # Calculate total pixel error (sum) for each df_error and print it\n",
    "    total_pixel_error = df_error.sum().sum()\n",
    "    print(f\"Total pixel error for model {model_name}: {total_pixel_error}\")\n",
    "\n",
    "    for i, kp in enumerate(df_error.columns):\n",
    "        df_w_vars.append(pd.DataFrame({\n",
    "            'pixel_error': df_error[kp],\n",
    "            'ens-std': ens_stddev[:, i],\n",
    "            'ens-std-prctile': [np.sum(ens_stddev < p) / ens_stddev.size for p in ens_stddev[:, i]],\n",
    "            'ens-std-prctile-kp': [np.sum(ens_stddev[:, i] < p) / ens_stddev[:, i].size for p in ens_stddev[:, i]], \n",
    "            'keypoint': kp,\n",
    "            'model': model_name,\n",
    "        }, index=df_error.index))\n",
    "\n",
    "df_w_vars = pd.concat(df_w_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_vals = np.arange(0, 5, 0.4)\n",
    "\n",
    "n_points_dict = {m: np.nan * np.zeros_like(std_vals) for m in model_names_list}\n",
    "df_line2 = []\n",
    "for s, std in enumerate(std_vals):\n",
    "    df_tmp_ = df_w_vars[df_w_vars['ens-std'] > std]\n",
    "    for model_name in model_names_list:\n",
    "        d = df_tmp_[df_tmp_.model == model_name]\n",
    "        n_points = np.sum(~d['pixel_error'].isna())\n",
    "        n_points_dict[model_name][s] = n_points\n",
    "        index = []\n",
    "        rng = 0\n",
    "        for row, k in zip(d.index, d['keypoint'].to_numpy()):\n",
    "            index.append(row + f'_{model_name}_{s}_{k}_{rng}')\n",
    "        df_line2.append(pd.DataFrame({\n",
    "            'ens-std': std,\n",
    "            'model': model_name,\n",
    "            'mean': d.pixel_error.to_numpy(),\n",
    "            'n_points': n_points,\n",
    "        }, index=index))\n",
    "df_line2 = pd.concat(df_line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4736768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Define the color mapping\n",
    "color_mapping = {\n",
    "    'networks': 'pink',\n",
    "    'eks': 'red',\n",
    "    'ens-mean': 'lightgreen',\n",
    "    'ens-median': 'green',\n",
    "    'aeks_hand_rng': 'lightblue',\n",
    "    'aeks_hand_eks': 'blue',\n",
    "    'aeks_ens-mean': 'orange',\n",
    "    'aeks_ens-median': 'red'\n",
    "}\n",
    "\n",
    "# Create a color list based on the model names\n",
    "def get_color(name):\n",
    "    if 'aeks_ens-median' in name:\n",
    "        return color_mapping['aeks_ens-median']\n",
    "    elif 'aeks_ens-mean' in name:\n",
    "        return color_mapping['aeks_ens-mean']\n",
    "    elif 'aeks_hand_rng' in name:\n",
    "        return color_mapping['aeks_hand_rng']\n",
    "    elif 'aeks_hand_eks' in name:\n",
    "        return color_mapping['aeks_hand_eks']\n",
    "    else:\n",
    "        base_name = name.split('_')[0]\n",
    "        return color_mapping.get(base_name, 'gray')\n",
    "\n",
    "palette = [get_color(name) for name in df_line2['model'].unique()]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))  # Increase the figure size\n",
    "\n",
    "g = sns.lineplot(\n",
    "    x='ens-std',\n",
    "    y='mean',\n",
    "    hue='model',\n",
    "    hue_order=model_names_list,\n",
    "    palette=palette,\n",
    "    data=df_line2,\n",
    "    ax=ax,\n",
    "    errorbar='se'\n",
    ")\n",
    "\n",
    "labels_fontsize = 12\n",
    "\n",
    "ax.set_title(f'Ground-Truth Pixel Error vs Ensemble Stdev (OOD)', fontsize=labels_fontsize)\n",
    "ax.set_ylabel('Pixel error', fontsize=labels_fontsize)\n",
    "ax.set_xlabel('Ensemble std dev', fontsize=labels_fontsize)\n",
    "cleanaxis(ax)\n",
    "\n",
    "# Customize legend\n",
    "legend_labels = {\n",
    "    'networks': 'Networks',\n",
    "    'eks': 'EKS',\n",
    "    'ens-mean': 'Ensemble Mean',\n",
    "    'ens-median': 'Ensemble Median',\n",
    "    'aeks_hand_rng': 'aEKS_hand',\n",
    "    'aeks_hand_eks': 'aEKS_hand EKS',\n",
    "    'aeks_ens-mean': 'aEKS Ensemble Mean',\n",
    "    'aeks_ens-median': 'aEKS Ensemble Median'\n",
    "}\n",
    "\n",
    "# Combine similar models into single legend entries\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "unique_labels = []\n",
    "unique_handles = []\n",
    "\n",
    "for handle, label in zip(handles, labels):\n",
    "    if 'aeks_ens-median' in label:\n",
    "        base_label = 'aeks_ens-median'\n",
    "    elif 'aeks_ens-mean' in label:\n",
    "        base_label = 'aeks_ens-mean'\n",
    "    elif 'aeks_hand_rng' in label:\n",
    "        base_label = 'aeks_hand_rng'\n",
    "    elif 'aeks_hand_eks' in label:\n",
    "        base_label = 'aeks_hand_eks'\n",
    "    else:\n",
    "        base_label = label.split('_')[0]\n",
    "\n",
    "    if base_label not in unique_labels:\n",
    "        unique_labels.append(base_label)\n",
    "        unique_handles.append(handle)\n",
    "\n",
    "ax.legend(\n",
    "    handles=unique_handles,\n",
    "    labels=[legend_labels.get(label, label) for label in unique_labels],\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1, 1),\n",
    "    fontsize=labels_fontsize,\n",
    "    title='model',\n",
    "    title_fontsize=labels_fontsize\n",
    ")\n",
    "\n",
    "# Plot annotations\n",
    "percentiles = [100, 50, 1]\n",
    "vals, prctiles = compute_percentiles(\n",
    "    arr=n_points_dict[model_names_list[0]],\n",
    "    std_vals=std_vals,\n",
    "    percentiles=percentiles,\n",
    ")\n",
    "\n",
    "for p, v in zip(prctiles, vals):\n",
    "    ax.axvline(v, ymax=0.95, linestyle='--', linewidth=0.5, color='k', zorder=-1)\n",
    "    ax.text(\n",
    "        v / np.diff(ax.get_xlim()), 0.95, str(round(p)) + '%',\n",
    "        transform=ax.transAxes,\n",
    "        ha='left',\n",
    "    )\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "pose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
